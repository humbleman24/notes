## 使用Transformer上的开源大模型进行微调

### 模型下载与权重下载

一般来说，模型会在transformers库中，可以到环境的Lib\site-package\transformers文件夹下找到对应的模型文件

对应的模型会有一个对应的Config文件，下面会提到如何查看对应的config文件

而权重则是从huggingface或modelscope中下载到本地，就可以直接在本地使用

### 模型与tokenizer导入

可以根据huggingface上的使用教程来导入相应的模型与tokenizer

```python
# 创建模型实例
model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype="auto",device_map="auto")

# 取出对应的模型词嵌入
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

通过这个方式导入的model其实就是pytorch下的一个模型，可以像nn.module一样使用

tokenizer的使用方法也比较简单，下面会提及

#### 查看模型的Config

在导入模型后，可以直接使用model.config来查看模型的信息

```
Qwen2Config {
  "_name_or_path": "D:/models/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}
```

比较关键的变量有：

- vocab_size: 这个模型的词汇表的大小，一般就是一个索引值，每个索引值都可以对应到相应的embedding上
- hidden_size：可以理解为embedding的维度
- intermediate_size：这个是MLP中升维的维度大小
- num_attention_heads：每一层q的个数
- num_key_value_heads：每一层k，v的个数
- num_hidden_layer：decoder的层数

### 构建dataset

一般来说，构建的dataset，要对input和output进行tokenizer，得到对应的sequence。

因为模型的输入应该是tokenizer之后的序列，然后由模型进行embedding        维度：（batch_size, sequence_length）

模型在计算cross entropy的时候，也是根据对应的概率与index做计算               维度：（batch_size, sequence_length）

**这里我还不太确定我做的对不对！下面简单介绍一下我的处理**

在进行tokinzer的时候，输出的结果需要统一到一个sequence length上去，否则dataloader无法进行处理

这就涉及到两种操作：

1. 当sequence length过短时要进行padding
2. 当sequence length过长是要进行truncate

```python
input_token = tokenizer(input_data, return_tensors="pt", padding="max_length", truncation=True, max_length=sequence_length)
```

我使用了这一个函数来进行上述的操作。

接下来得到的return值其实是一个字典

```json
{
    "input_ids" : tensor,
    "attention_mask" : tensor
}
```

可以看到，在进行了tokenizer后，返回了一个tensor存放的是index

而attention_mask是将padding设置为“max_length”才会有的一个值，为的是告诉模型，后面的部分是padding的，不应该考虑到attention中

对于这个attention_mask，在input和output的处理上也是有不同的

1. 因为forward函数是可以传入input的attention_mask，所以dataset的返回函数可以直接将tokenizer的tensor和attention的tensor一并传出
2. 而根据Qwen2模型的注释，label由于没有对应的attention_mask的输入，应将不应该被注意到部分设置为-100，所以也就有了接下来的操作

```python
output_token[output_masking == 0] = -100     # 变量已经取出
```

### 简单的对模型进行操作

这里我对模型的操作仅限于将低层的权重进行冻结，只训练最后几层以适应下游目标任务

方法也很简单

```python
for name, param in model.named_parameters():
    # split out the layer number
    name_l = name.split('.')
    layer_num = name_l[2] if name_l[1] == 'layers' else None
    
    if layer_num is not None:
        # frozen the lower layers to preserve the features
        # train the higher layers to fit into the specific downstream task
        layer_num = int(layer_num)
        if layer_num < 23:
            param.requires_grad = False
        elif layer_num >= 23:
            param.requires_grad = True
```

### 开始训练

完成上述的步骤后，应该就可以进行正常的模型训练了

首先配置超参数，然后加载dataloader，构建optimizer，就可以进行for循环训练模型了!

但是现在遇到的一些不太寻常的问题，不知道是否正常

1. 在遍历dataloader的时候，取出对应的元素的使用，中间加了一个维度，也就是变成了（batch_size, 1, sequence_length）
2. 在forward的时候，出现了显存爆炸，不知道为什么

#### 尝试解决显存爆炸

多冻结两层，失败

尝试减少sequence长度，失败

强制使用半精度，失败

现在检查到的原因十分尴尬，到第6层的时候，就已经满显存了，所以简单的处理方法已经不行了

