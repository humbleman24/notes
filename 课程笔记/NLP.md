# Natural Language Processing

### Context Free Grammars

a CFGs is a system that  uses specific rules to show **how sentences** in a language **are constructed**. These rules define **how elements of a sentence are combined**.

context-free indicates that these rules does **not** depend **on the surrounding symbols**!

- Non-terminals: these are abstract symbols that represent groups of words, used to structure the sentence but don't appear in the actual text.

  sentences (S), noun phrases (NP), verb phrases (VP)

- terminals: These are the actual words in the language.

- production rules: these rules show how to build sentences by replacing non-terminals with terminals or other non-terminals

A CFG specifies a formal language. Sentences can be generated by the grammar are considered "grammatical" in the formal language, "ungrammatical" otherwise.

It is useful in NLP for:

- parsing: analysing the structure of sentences to understand their grammar
- grammar checking: identifying mistakes in sentences by checking them against grammar rules
- sentence generation, creating new sentences that are grammatically correct.

#### CKY Parsing

Chomsky normal form is a specific way of structing CFGs.

Cocke-Younger-Kasami algorithm is a parsing algorithm, used for **syntactic analysis of sentences** according to a given grammar in CNF, determine whether a given sentence **can be generated by the grammar** and, if so, to construct a corresponding parse tree.

<img src="F:\notes\pic\CKY-parse.png" style="zoom:50%;" />

### Language Model

A language model is a computational system used to **compute the probability** of a sentence or sequence of words.
$$
\begin{align}
	P(W) = P(W_1, W_2, ... , W_n)
\end{align}
$$
how likely a word or sequence of words is to appear in a given text. 

related task: probability of an upcoming word: $P(W_n | W_1, W_2, ... , W_n)$

The chain rule in general: 
$$
\prod_{i} P(W_i|W_1, W_2,...,W_n)
$$
word probability is estimated by word frequency, counting how frequently a word appears in a text corpus divided by the total number of words in the corpus.

To predict the probability of next words, but there are too many combination of words and we'll never see enough data for estimating the long combinations!  ------

**Markov Assumption**

simplifying assumption: the probability of a word depends only on the preceding $n-1$ words, not on any earlier context.

#### N-gram Models

In general, this is an in sufficient model of language because language has long-distance dependencies

##### Corpus

corpora are a large and structured set of texts.

corpora are used to define a language or sub-language

corpora are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.

- large scale: contains extensive text data, covering a wide range of linguistic phenomona.
- structured: organised according to certain rules, making it easy to retrieve and analyse
- representative: texts come from diverse sources, aiming to reflect the real use of the target language

However, in N-gram model, one term is 0, it would cause the entire sentence probability to be 0.

Apply smoothing algorithm

##### Add-one (Laplace Smoothing)

add a small constant (usually 1) to all event counts. This ensures that every possible event has a non-zero probability.









































